# Proyecto Final â€“ AutomatizaciÃ³n QA ğŸš€

Entrega final del **Proyecto de AutomatizaciÃ³n QA**, desarrollado con **Python + Selenium + Pytest**, aplicando buenas prÃ¡cticas de testing automatizado.

---

## ğŸ“Œ Objetivo del proyecto

Este proyecto tiene como objetivo demostrar la automatizaciÃ³n de pruebas funcionales
sobre un sitio web demo, utilizando buenas prÃ¡cticas como:

* Page Object Model (POM)
* SeparaciÃ³n entre lÃ³gica de pruebas y lÃ³gica de interacciÃ³n con la UI
* Data-driven testing
* GeneraciÃ³n de evidencias automÃ¡ticas ante fallos

El sitio utilizado para las pruebas es:
ğŸ”— [https://www.saucedemo.com](https://www.saucedemo.com)

---

## ğŸ› ï¸ TecnologÃ­as utilizadas

* Python 3
* Selenium WebDriver
* Pytest
* Pytest-HTML
* WebDriver Manager
* CSV (datos de prueba)

---

## ğŸ“‚ Estructura del proyecto

```text
pages/          â†’ Page Objects (interacciÃ³n con la UI)
tests/          â†’ Casos de prueba
utils/          â†’ Utilidades (logger)
data/           â†’ Datos de prueba (CSV)
reports/        â†’ Reporte HTML y screenshots
```

---

## ğŸ§© Page Object Model (POM)

El proyecto implementa el patrÃ³n **Page Object Model**, separando claramente:

* La lÃ³gica de los tests
* De la lÃ³gica de interacciÃ³n con la interfaz de usuario

Cada pÃ¡gina del sistema cuenta con su propia clase dentro del directorio `pages/`:

* LoginPage
* InventoryPage
* CartPage
* CheckoutPage

Los tests interactÃºan Ãºnicamente con mÃ©todos de estas clases, logrando:

* Tests mÃ¡s legibles
* CÃ³digo mantenible
* ReutilizaciÃ³n de lÃ³gica

---

## ğŸ” Data-driven testing

El login se ejecuta utilizando datos externos desde un archivo **CSV**, permitiendo probar
mÃºltiples escenarios (vÃ¡lidos e invÃ¡lidos) sin modificar el cÃ³digo del test.

---

## ğŸ“¸ Evidencias automÃ¡ticas (Screenshots)

Cuando una prueba falla, el sistema captura automÃ¡ticamente una **screenshot** del navegador.

Las capturas se almacenan en:

```text
reports/screenshots/
```

El nombre del archivo incluye:

* Fecha y hora
* Nombre del test que fallÃ³

Esto facilita el anÃ¡lisis y la documentaciÃ³n de errores.

---

## â–¶ï¸ InstalaciÃ³n y ejecuciÃ³n

### 1ï¸âƒ£ Clonar el repositorio

```bash
git clone https://github.com/Marisa2191/Pre-entrega-Marisa-Gonzales.git
cd Pre-entrega-Marisa-Gonzales
```

### 2ï¸âƒ£ Crear y activar entorno virtual

```bash
python -m venv venv
source venv/bin/activate
```

### 3ï¸âƒ£ Instalar dependencias

```bash
pip install -r requirements.txt
```

---

## â–¶ï¸ EjecuciÃ³n de pruebas

Ejecutar todos los tests:

```bash
pytest -v
```

Generar reporte HTML:

```bash
pytest --html=reports/reporte.html --self-contained-html
```

El reporte se genera en:

```text
reports/reporte.html
```

---

## ğŸ§ª Casos de prueba implementados

* Login exitoso y fallido
* ValidaciÃ³n del catÃ¡logo de productos
* ObtenciÃ³n de nombre y precio de productos
* InteracciÃ³n con el carrito de compras


---

## ğŸŒ Pruebas de API (Requests)

AdemÃ¡s de las pruebas UI con Selenium, el proyecto incluye pruebas de API utilizando la librerÃ­a **requests**, validando:

- CÃ³digos de estado HTTP
- Estructura de respuestas JSON
- Escenarios de Ã©xito y error
- Encadenamiento de peticiones (POST â†’ GET)

### ğŸ“ UbicaciÃ³n
Los tests de API estÃ¡n en la carpeta:

tests_api/

### âœ… Casos implementados (API pÃºblica: JSONPlaceholder)

**1) GET /posts (Ã©xito)**
- Verifica `status_code == 200`
- Valida que la respuesta sea una lista y que tenga elementos

Archivo:
- `tests_api/test_api_jsonplaceholder.py`

**2) POST /posts (Ã©xito)**
- Verifica `status_code in (200, 201)`
- Valida que la respuesta sea JSON (dict)
- Verifica que devuelva un `id` y que refleje campos enviados (title/body/userId)

Archivo:
- `tests_api/test_api_post_posts.py`

**3) POST /posts (error en endpoint)**
- EnvÃ­a la peticiÃ³n a un endpoint invÃ¡lido
- Verifica que el status sea de error (ej: 404)

Archivo:
- `tests_api/test_api_post_posts.py`

**4) DELETE /posts/{id}**
- Verifica un status esperado de borrado (ej: 200 o 204)
- Tolera respuesta vacÃ­a o `{}` (segÃºn comportamiento del servicio)

Archivo:
- `tests_api/test_api_delete_posts.py`

### ğŸ”— Encadenamiento de peticiones (Opcional)
Se implementa un flujo donde una peticiÃ³n depende de otra:

1. Se crea un recurso con **POST**
2. Se usa el `id` devuelto para intentar obtenerlo con **GET**

> Nota: JSONPlaceholder es un servicio â€œfakeâ€, por lo que puede devolver un `id` creado pero no persistirlo realmente.
> Por eso el test valida el `id` del POST y maneja la respuesta del GET de forma esperable.

Archivo:
- `tests_api/test_api_chain_post_get.py`

### â–¶ï¸ CÃ³mo ejecutar (API)

Ejecutar solo API:
```bash
pytest tests_api -v

Ejecutar solo encadenamiento POST â†’ GET:

pytest tests_api/test_api_chain_post_get.py -v

Ejecutar todo el proyecto (UI + API):

pytest -v
---
## ğŸ“Š Estado del proyecto

âœ”ï¸ Login automatizado
âœ”ï¸ CatÃ¡logo de productos
âœ”ï¸ InteracciÃ³n con carrito
âœ”ï¸ Page Object Model
âœ”ï¸ Data-driven testing
âœ”ï¸ Evidencias automÃ¡ticas

ğŸ **Proyecto finalizado y listo para su evaluaciÃ³n.**

---

## ğŸ‘©â€ğŸ’» Autora

**Marisa Gonzales**
QA Analyst â€“ Automation Testing
